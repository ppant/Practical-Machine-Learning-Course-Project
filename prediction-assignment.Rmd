---
title: "Course Project Prediction assignment"
author: "Pradeep K. Pant, ppant@cpan.org"
date: "July 26, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## 1. Introduction
For this project, we are given data from accelerometers on the belt, forearm, arm, and dumbell of 6 research study participants. Our training data consists of accelerometer data and a label identifying the quality of the activity the participant was doing. Our testing data consists of accelerometer data without the identifying label. Our goal is to predict the labels for the test set observations.

Later part of the project I have shown the way I have built the model, coded for cross validation, estimating the out-of-sample error and making predictions.

## 2. Prepating data 

```{r}
# Loading and preprocessing the data
# Load CRAN modules 
library(downloader)
library(plyr)
library(knitr)
library(datasets)
library(ggplot2)
library(rmarkdown)
library(caret)

# Step 1
# Download the training data set if not avaliable in default location
train_Url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
# Download the test data set if not avaliable in default location
test_Url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

# Train data
# Check if zip has already been downloaded in projectData directory?
if(!file.exists("train_data.csv")){
  download.file(train_Url,destfile="train_data.csv",mode = "wb")
  }
# Test data
# Check if zip has already been downloaded in projectData directory?
if(!file.exists("test_data.csv")){
  download.file(test_Url,destfile="test_data.csv",mode = "wb")
  }
# Read the .CSV file in R data structure 
trainData <- read.csv("train_data.csv")
testData <- read.csv("test_data.csv")
# Split the data in traning and validation point based on classe variable
set.seed(123)
inTrain <- createDataPartition(y=trainData$classe, p=0.7, list=F)
trainData1 <- trainData[inTrain, ]
trainData2 <- trainData[-inTrain, ]
```
In the next step we will be reducing number of features by removing variables with nearly zero variance, variables that are almost always NA, and variables that don't add much value in making prediction. We'll then analyse trainData1 and trainData2 and then will decide which ones to remove.
```{r}
# Lets remove variables with nearly zero variance
nzv <- nearZeroVar(trainData1)
trainData1 <- trainData1[, -nzv]
trainData2 <- trainData2[, -nzv]

# remove variables that are almost always NA
mostlyNA <- sapply(trainData1, function(x) mean(is.na(x))) > 0.95
trainData1 <- trainData1[, mostlyNA==F]
trainData2 <- trainData2[, mostlyNA==F]

# remove variables that don't add much value addition to the prediction, which are the first five variables
trainData1 <- trainData1[, -(1:5)]
trainData2 <- trainData2[, -(1:5)]
```

## 3. Building a model 
I have used Random Forest model, to see if it would have acceptable performance. First, fit the model on trainData1, and instruct the "train" function to use 3-fold cross-validation to select optimal tuning parameters for the model.
```{r}
library(randomForest)
# train using 3-fold Cross validation to select optimal tuning parameters
fitControl <- trainControl(method="cv", number=3, verboseIter=F)

# fit model on trainData1 using Random forest model
fit <- train(classe ~ ., data=trainData1, method="rf", trControl=fitControl)
# print the final model and see which parameters has been used 
fit$finalModel
```
## 4. Evaluating model and predict label
Now, I use the fitted model to predict the label ("classe") in trainData2, and show the confusion matrix to compare the predicted versus the actual labels:
```{r}
# Now, use model to predict classe in validation set (trainData2)
preds <- predict(fit, newdata=trainData2)

# show confusion matrix to get estimate of out-of-sample error
confusionMatrix(trainData2$classe, preds)
```
The accuracy is 99.82% and predicted accuracy for the out-of-sample error is 0.28%. So Random Forests alogorithm looks good to predict on the test set.

##5. Re-training the Model on main training and test data
One important point before predicting on the test set, that we the train the model on the full training set (trainData), rather than using a model trained on a reduced training set (trainData1), in order to produce the most accurate predictions. So we'll repeat the same on original train and test set (trainData and testData respectively):
```{r}
# remove variables with nearly zero variance
nzv <- nearZeroVar(trainData)
trainData <- trainData[, -nzv]
testData <- testData[, -nzv]

# remove variables that are almost always NA
mostlyNA <- sapply(trainData, function(x) mean(is.na(x))) > 0.95
trainData <- trainData[, mostlyNA==F]
testData <- testData[, mostlyNA==F]

# remove variables that don't add much value addition to the prediction, which are the first five variables
trainData <- trainData[, -(1:5)]
testData <- testData[, -(1:5)]

# re-fit model using full training set (trainData)
fitControl <- trainControl(method="cv", number=3, verboseIter=F)
fit <- train(classe ~ ., data=trainData, method="rf", trControl=fitControl)
```

## 6. Making Test Set Predictions
Finally, we'll use the model fit on trainData to predict the label for the observations in testData, and write those predictions to individual files:
```{r}
# predict on test set
predictTestSet <- predict(fit, newdata=testData)

# convert predictions to character vector
predictTestSet <- as.character(predictTestSet)

# create function to write predictions to files
pml_write_files <- function(x) {
    n <- length(x)
    for(i in 1:n) {
        filename <- paste0("test_case_", i, ".txt")
        write.table(x[i], file=filename, quote=F, row.names=F, col.names=F)
    }
}

# create prediction files to submit
pml_write_files(predictTestSet)
```
